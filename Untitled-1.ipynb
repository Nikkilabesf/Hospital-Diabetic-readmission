{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef39bbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 175)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:175\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mstart.click(start_chat, outputs=[chat, inp, state])\u001b[39m\n                                                       ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590689ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-14 07:21:08.593342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/tmp/ipykernel_174075/456214311.py:95: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat = gr.Chatbot(height=440, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://e19d7cfbd6cc75a038.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e19d7cfbd6cc75a038.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app_tf.py — Diabetes Readmit Chat (TensorFlow)\n",
    "# - Fixed first-question loop\n",
    "# - Auto free-port selection\n",
    "# - Public link by default (share=True)\n",
    "\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from joblib import load\n",
    "\n",
    "# ---------- Load model + preprocessor ----------\n",
    "MODEL = tf.keras.models.load_model(\"tf_model.h5\")\n",
    "PRE = load(\"tf_preprocessor.joblib\")\n",
    "\n",
    "# ---------- Columns (must match training) ----------\n",
    "CAT_COLS = [\"race\", \"gender\", \"age\", \"A1Cresult\", \"insulin\", \"change\", \"diabetesMed\"]\n",
    "NUM_COLS = [\n",
    "    \"time_in_hospital\",\n",
    "    \"num_lab_procedures\",\n",
    "    \"num_procedures\",\n",
    "    \"num_medications\",\n",
    "    \"number_outpatient\",\n",
    "    \"number_emergency\",\n",
    "    \"number_inpatient\",\n",
    "]\n",
    "\n",
    "# ---------- Chat flow ----------\n",
    "TRIAGE_QUESTIONS = [\n",
    "    (\"chest_pain\",        \"Chest pain or shortness of breath? (Yes/No)\"),\n",
    "    (\"confusion\",         \"Confusion, dizziness, or disorientation? (Yes/No)\"),\n",
    "    (\"blood_sugar_swings\",\"Extremely high or low blood sugar recently? (Yes/No)\"),\n",
    "    (\"infection\",         \"Infection or non-healing wound? (Yes/No)\"),\n",
    "    (\"dehydration\",       \"Severe fatigue or dehydration? (Yes/No)\"),\n",
    "    (\"med_nonadherence\",  \"Missing diabetes meds/insulin doses? (Yes/No)\"),\n",
    "]\n",
    "\n",
    "MODEL_QUESTIONS = [\n",
    "    (\"age\", \"Age bracket? (e.g., [50-60), [60-70), [70-80))\"),\n",
    "    (\"gender\", \"Gender? (Male/Female/Unknown/Other)\"),\n",
    "    (\"race\", \"Race? (Caucasian/AfricanAmerican/Asian/Hispanic/Other/Unknown)\"),\n",
    "    (\"A1Cresult\", \"Latest A1C result? (None/Norm/>7/>8)\"),\n",
    "    (\"insulin\", \"On insulin? (No/Steady/Up/Down)\"),\n",
    "    (\"change\", \"Any diabetes-med change this encounter? (Ch/No)\"),\n",
    "    (\"diabetesMed\", \"On any diabetes meds? (Yes/No)\"),\n",
    "    (\"time_in_hospital\", \"Days in hospital this encounter? (number)\"),\n",
    "    (\"num_medications\", \"Number of medications? (number)\"),\n",
    "    (\"number_emergency\", \"Emergency visits in prior year? (number)\"),\n",
    "]\n",
    "\n",
    "NORMALIZE = {\n",
    "    \"A1Cresult\": {\">7\":\">7\",\">8\":\">8\",\"none\":\"None\",\"norm\":\"Norm\",\"normal\":\"Norm\"},\n",
    "    \"insulin\": {\"yes\":\"Steady\",\"no\":\"No\",\"steady\":\"Steady\",\"up\":\"Up\",\"down\":\"Down\"},\n",
    "    \"change\": {\"yes\":\"Ch\",\"no\":\"No\",\"ch\":\"Ch\"},\n",
    "    \"diabetesMed\": {\"yes\":\"Yes\",\"no\":\"No\"},\n",
    "    \"gender\": {\"m\":\"Male\",\"male\":\"Male\",\"f\":\"Female\",\"female\":\"Female\"},\n",
    "}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def empty_state() -> Dict[str, Any]:\n",
    "    # phases: 'triage' -> 'model'\n",
    "    return {\"answers\": {}, \"idx\": 0, \"phase\": \"triage\"}\n",
    "\n",
    "def parse_num(s: str) -> int:\n",
    "    m = re.search(r\"-?\\d+\", s or \"\")\n",
    "    return int(m.group()) if m else 0\n",
    "\n",
    "def norm(name: str, s: str) -> str:\n",
    "    t = (s or \"\").strip()\n",
    "    m = NORMALIZE.get(name)\n",
    "    if m:\n",
    "        return m.get(t.lower(), t)\n",
    "    return t\n",
    "\n",
    "def yesno(s: str) -> str:\n",
    "    return \"Yes\" if (s or \"\").strip().lower() in {\"y\",\"yes\",\"1\",\"true\"} else \"No\"\n",
    "\n",
    "def triage_score(ans: Dict[str, Any]) -> int:\n",
    "    keys = [k for k, _ in TRIAGE_QUESTIONS]\n",
    "    return sum(1 for k in keys if ans.get(k, \"No\") == \"Yes\")\n",
    "\n",
    "def build_model_row(answers: Dict[str, Any]) -> pd.DataFrame:\n",
    "    # Fill missing categoricals with \"Unknown\" and numerics with 0\n",
    "    row = {**{c: answers.get(c, \"Unknown\") for c in CAT_COLS},\n",
    "           **{n: answers.get(n, 0) for n in NUM_COLS}}\n",
    "    return pd.DataFrame([row], columns=CAT_COLS + NUM_COLS)\n",
    "\n",
    "# ---------- UI ----------\n",
    "def build_app():\n",
    "    with gr.Blocks(title=\"Diabetes Readmit Screener\", fill_height=True, theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"### 🩺 Diabetes Readmit Screener (TensorFlow)\\n\"\n",
    "                    \"First I’ll check urgent symptoms, then estimate **Readmit: Yes/No**.\")\n",
    "\n",
    "        chat = gr.Chatbot(height=440, show_copy_button=True)\n",
    "        inp = gr.Textbox(placeholder=\"Type your answer and press Enter…\")\n",
    "        start = gr.Button(\"Start\", variant=\"primary\")\n",
    "        state = gr.State(empty_state())\n",
    "\n",
    "        def start_chat():\n",
    "            st = empty_state()\n",
    "            return [], TRIAGE_QUESTIONS[0][1], st\n",
    "\n",
    "        # ----- FIXED: first-answer handling so it advances to Q2 -----\n",
    "        def respond(message, history, st):\n",
    "            # Always log user message first\n",
    "            history.append((\"You\", message))\n",
    "\n",
    "            # ===== TRIAGE PHASE =====\n",
    "            if st[\"phase\"] == \"triage\":\n",
    "                # If first interaction:\n",
    "                if st[\"idx\"] == 0 and not st[\"answers\"]:\n",
    "                    # If user typed a start word or blank -> ask Q1\n",
    "                    if message.strip().lower() in {\"\", \"start\", \"hi\", \"hello\"}:\n",
    "                        history.append((\"Assistant\", TRIAGE_QUESTIONS[0][1]))\n",
    "                        return \"\", history, st\n",
    "                    # Else treat their first reply as the answer to Q1\n",
    "                    key0 = TRIAGE_QUESTIONS[0][0]\n",
    "                    st[\"answers\"][key0] = yesno(message)\n",
    "                    st[\"idx\"] = 1\n",
    "                    if st[\"idx\"] < len(TRIAGE_QUESTIONS):\n",
    "                        history.append((\"Assistant\", TRIAGE_QUESTIONS[st[\"idx\"]][1]))\n",
    "                        return \"\", history, st\n",
    "                else:\n",
    "                    # Normal triage flow (Q2..Qn)\n",
    "                    key = TRIAGE_QUESTIONS[st[\"idx\"]][0]\n",
    "                    st[\"answers\"][key] = yesno(message)\n",
    "                    st[\"idx\"] += 1\n",
    "                    if st[\"idx\"] < len(TRIAGE_QUESTIONS):\n",
    "                        history.append((\"Assistant\", TRIAGE_QUESTIONS[st[\"idx\"]][1]))\n",
    "                        return \"\", history, st\n",
    "\n",
    "                # Triage complete → move to model phase\n",
    "                st[\"phase\"] = \"model\"\n",
    "                st[\"idx\"] = 0\n",
    "                history.append((\"Assistant\", \"Thanks. Now a few quick background questions.\"))\n",
    "                history.append((\"Assistant\", MODEL_QUESTIONS[0][1]))\n",
    "                return \"\", history, st\n",
    "\n",
    "            # ===== MODEL PHASE =====\n",
    "            key, _ = MODEL_QUESTIONS[st[\"idx\"]]\n",
    "            if key in {\"time_in_hospital\",\"num_lab_procedures\",\"num_procedures\",\n",
    "                       \"num_medications\",\"number_outpatient\",\"number_emergency\",\"number_inpatient\"}:\n",
    "                st[\"answers\"][key] = parse_num(message)\n",
    "            else:\n",
    "                st[\"answers\"][key] = norm(key, message)\n",
    "\n",
    "            st[\"idx\"] += 1\n",
    "            if st[\"idx\"] < len(MODEL_QUESTIONS):\n",
    "                history.append((\"Assistant\", MODEL_QUESTIONS[st[\"idx\"]][1]))\n",
    "                return \"\", history, st\n",
    "\n",
    "            # ===== FINAL: Predict =====\n",
    "            score = triage_score(st[\"answers\"])\n",
    "            if score >= 3:\n",
    "                banner = f\"⚠️ **High symptom concern** ({score} red flags) — consider urgent review.\"\n",
    "            elif score == 2:\n",
    "                banner = f\"🟠 **Moderate symptom concern** (2 red flags).\"\n",
    "            else:\n",
    "                banner = f\"✅ **Low symptom concern** ({score} red flags).\"\n",
    "\n",
    "            X = build_model_row(st[\"answers\"])\n",
    "            Xp = PRE.transform(X)\n",
    "            proba_yes = float(MODEL.predict(Xp, verbose=0)[0][0])\n",
    "            pred = \"Yes\" if proba_yes >= 0.5 else \"No\"\n",
    "            conf = proba_yes if pred == \"Yes\" else 1 - proba_yes\n",
    "            msg = f\"{banner}\\n\\n**Readmit: {pred}** (confidence {conf:.2f})\"\n",
    "\n",
    "            history.append((\"Assistant\", msg))\n",
    "            return \"\", history, empty_state()\n",
    "\n",
    "        start.click(start_chat, outputs=[chat, inp, state])\n",
    "        inp.submit(respond, inputs=[inp, chat, state], outputs=[inp, chat, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "# ---------- Launch (auto free port; public link on) ----------\n",
    "if __name__ == \"__main__\":\n",
    "    app = build_app()\n",
    "    app.queue()\n",
    "    # server_port=None lets Gradio pick any free port automatically\n",
    "    app.launch(server_name=\"0.0.0.0\", server_port=None, share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ba189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-14 07:14:05.990464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/workspaces/Hospital-Diabetic-readmission/app_tf.py:92: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat = gr.Chatbot(height=440, show_copy_button=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Hospital-Diabetic-readmission/app_tf.py\", line 167, in <module>\n",
      "    app.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/blocks.py\", line 2635, in launch\n",
      "    ) = http_server.start_server(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/gradio/http_server.py\", line 157, in start_server\n",
      "    raise OSError(\n",
      "OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n"
     ]
    }
   ],
   "source": [
    "!python app_tf.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
